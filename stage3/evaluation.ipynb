{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.ops as ops  # For NMS\n",
    "from utils import get_iou, state_dict_cleaner, detr2xyxy, coco2xyxy, xyxy2coco, get_psuedo_knn, process_target, get_table_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\remote desktop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GNLightning(\n",
       "  (gnet): GraphNetwork(\n",
       "    (conv1): GCNConv(256, 1024)\n",
       "    (conv2): GCNConv(1024, 1024)\n",
       "    (conv3): GCNConv(1024, 1024)\n",
       "    (lin1): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (lin_final): Linear(in_features=1024, out_features=3, bias=True)\n",
       "  )\n",
       "  (criterion): NLLLoss()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import GNLightning\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gnet = GNLightning.load_from_checkpoint(\n",
    "                                    checkpoint_path=r\"..\\checkpoints\\gnet_stage3.ckpt\",\n",
    "                                    d_model=1024,\n",
    "                                    lr=1e-3,\n",
    "                                    batch_size=2,\n",
    "                                    num_workers=0,\n",
    "                                    train_path=r'..\\misc\\placeholder.jsonl',\n",
    "                                    val_path=r'..\\misc\\placeholder.jsonl')\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "gnet.to(device)\n",
    "gnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this DETR model is only used for forward pass with the weights we trained from \n",
    "\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "class DetrLightning(LightningModule):\n",
    "    def __init__(self, model_name, checkpoint, num_labels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = DetrForObjectDetection.from_pretrained(model_name, num_labels=num_labels,ignore_mismatched_sizes=True)\n",
    "        state_dict = torch.load(checkpoint)['state_dict']\n",
    "        state_dict = state_dict_cleaner(state_dict)\n",
    "        self.model.load_state_dict(state_dict)\n",
    "    \n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "        input_labels,  \n",
    "        image_dir, \n",
    "        detr_model, \n",
    "        checkpoint,\n",
    "        num_labels=64,\n",
    "        iou_threshold=0.8, \n",
    "        confidence_threshold=0.5, \n",
    "        iou_match_threshold=0.5,\n",
    "        device = \"cuda\"):\n",
    "    \n",
    "    eval_values = []\n",
    "\n",
    "    # Set up the image processor and the DETR model\n",
    "    image_processor = DetrImageProcessor.from_pretrained(detr_model)\n",
    "    detr = DetrLightning(detr_model, checkpoint,num_labels)\n",
    "    detr.to(device)\n",
    "    detr.eval()\n",
    "\n",
    "    # Count total lines in the input file for progress tracking\n",
    "    with open(input_labels, 'r', encoding=\"utf8\") as infile:\n",
    "        total_lines = sum(1 for _ in infile)\n",
    "\n",
    "    # Progress bar setup\n",
    "    progress = tqdm(total=total_lines, desc=\"Processing Batches\", unit=\"samples\")\n",
    "\n",
    "    with open(input_labels, 'r', encoding=\"utf8\") as infile:\n",
    "\n",
    "        for line in infile:\n",
    "            # Parse the JSON line\n",
    "            label = json.loads(line)\n",
    "\n",
    "            # Update progress bar \n",
    "            progress.update(1)\n",
    "\n",
    "            # === FORWARD PASS THROUGH THE DETR MODEL TO GET THE PRED BBOXES AND HIDDEN STATES ===\n",
    "\n",
    "            image = Image.open(os.path.join(image_dir, label['filename'])).convert(\"RGB\")\n",
    "            inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "            # Forward pass through the model\n",
    "            outputs = detr(**inputs)\n",
    "\n",
    "            logits = outputs.logits[0]  # Shape: (batch_size, num_queries, num_classes)\n",
    "            pred_boxes = outputs.pred_boxes[0]  # Shape: (num_queries (100) , 4)\n",
    "            image_width, image_height = image.size  # Get original image dimensions\n",
    "            pred_boxes = torch.tensor(\n",
    "                [detr2xyxy(box.cpu().tolist(), image_width, image_height) for box in pred_boxes],\n",
    "                device=device\n",
    "            )\n",
    "            hidden_states = outputs.last_hidden_state[0]  # Shape: (batch_size, num_queries, hidden_dim)\n",
    "\n",
    "            # as per the docs\n",
    "            prob = F.softmax(logits, dim=-1)\n",
    "            scores, _ = prob[..., :-1].max(-1)\n",
    "\n",
    "            # Filter by confidence threshold\n",
    "            keep = scores > confidence_threshold\n",
    "            pred_boxes = pred_boxes[keep]\n",
    "            hidden_states = hidden_states[keep]\n",
    "            scores = scores[keep]\n",
    "\n",
    "            # Apply NMS\n",
    "            if pred_boxes.shape[0] > 0:\n",
    "                keep_nms = ops.nms(pred_boxes, scores, iou_threshold)\n",
    "                pred_boxes = pred_boxes[keep_nms]\n",
    "                hidden_states = hidden_states[keep_nms]\n",
    "\n",
    "            bbox_indices = []\n",
    "            filtered_boxes = []\n",
    "            filtered_hidden_states = []\n",
    "            gt_bboxes_coco = label['gt_bboxes']\n",
    "            gt_bboxes_xyxy = [coco2xyxy(box) for box in gt_bboxes_coco]\n",
    "            gt_bbox_indices = torch.tensor(label['gt_bbox_indices'], dtype=torch.int) \n",
    "\n",
    "            for pred_box, hidden_state in zip(pred_boxes, hidden_states):\n",
    "\n",
    "                pred_box = pred_box.detach().cpu().numpy().tolist()\n",
    "\n",
    "                # Find the ground truth box with the highest IoU\n",
    "                max_iou = 0\n",
    "                best_gt_index = -1\n",
    "                for i, gt_bbox in enumerate(gt_bboxes_xyxy):\n",
    "                    iou = get_iou(pred_box, gt_bbox)\n",
    "                    if iou > max_iou:\n",
    "                        max_iou = iou\n",
    "                        best_gt_index = i\n",
    "\n",
    "                # Keep the DETR bbox if the highest IoU exceeds the threshold\n",
    "                if max_iou >= iou_match_threshold:\n",
    "                    filtered_boxes.append(pred_box)\n",
    "                    filtered_hidden_states.append(hidden_state.detach().cpu().numpy().tolist())\n",
    "                    bbox_indices.append(gt_bbox_indices[best_gt_index])\n",
    "\n",
    "            filtered_boxes = torch.tensor([xyxy2coco(bbox) for bbox in filtered_boxes])\n",
    "            filtered_hidden_states = torch.tensor(filtered_hidden_states)\n",
    "\n",
    "            probs, edge_index = gnet(filtered_boxes, filtered_hidden_states)\n",
    "\n",
    "            probs, edge_index = probs.to('cpu'), edge_index.to('cpu')\n",
    "\n",
    "            # === GET THE GROUNTRUTH EDGE SET AS A DICT OF (START BBOX INDEX, END BBOX INDEX): CLASS ===\n",
    "\n",
    "            thead_grid, tbody_grid = get_table_grid(''.join(label['html']))\n",
    "            table_grid = thead_grid + tbody_grid\n",
    "\n",
    "            gt_edge_index = get_psuedo_knn(torch.tensor(gt_bboxes_coco))\n",
    "\n",
    "            gt_bbox_index_pairs = torch.stack((\n",
    "                gt_bbox_indices[gt_edge_index[0]],  # Start bounding boxes\n",
    "                gt_bbox_indices[gt_edge_index[1]]   # End bounding boxes\n",
    "            ), dim=1)\n",
    "\n",
    "            gt_classes = process_target(gt_bbox_index_pairs, table_grid) # dtype long\n",
    "\n",
    "            gt_edgeset = {}\n",
    "\n",
    "            # Iterate over the pairs and classes\n",
    "            for pair, gt_class in zip(gt_bbox_index_pairs, gt_classes):\n",
    "                start_bbox_index = pair[0].item()  # Convert to a standard Python integer\n",
    "                end_bbox_index = pair[1].item()   # Convert to a standard Python integer\n",
    "                gt_edgeset[(start_bbox_index, end_bbox_index)] = gt_class.item()  # Map to the class value \n",
    "\n",
    "            # === PARSE THROUGH EACH PRED EDGE AND CHECK IF IT MATCHES WITH THE GT EDGESET ===\n",
    "            numerator = 0\n",
    "            for i, (start, end) in enumerate(edge_index.t()):\n",
    "                \n",
    "                # Get the predicted class for the edge\n",
    "                predicted_class = torch.argmax(probs[i]).item()\n",
    "\n",
    "                # no relationship edge, skip\n",
    "                if predicted_class == 0: continue\n",
    "\n",
    "                if (bbox_indices[start].item(), bbox_indices[end].item()) in gt_edgeset and gt_edgeset[(bbox_indices[start].item(), bbox_indices[end].item())] == predicted_class:\n",
    "                    numerator+=1\n",
    "\n",
    "            denominator = sum(1 for value in gt_edgeset.values() if value != 0)\n",
    "\n",
    "            eval_values.append(numerator/denominator)\n",
    "\n",
    "            if progress.n == 50: break\n",
    "\n",
    "        # Close the progress bar\n",
    "        progress.close()\n",
    "\n",
    "    return eval_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DetrForObjectDetection were not initialized from the model checkpoint at facebook/detr-resnet-50 and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([65]) in the model instantiated\n",
      "- class_labels_classifier.weight: found shape torch.Size([92, 256]) in the checkpoint and torch.Size([65, 256]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\remote desktop\\AppData\\Local\\Temp\\ipykernel_7800\\2337314192.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(checkpoint)['state_dict']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212594e4654740f0a2217efcb24704a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Batches:   0%|          | 0/9081 [00:00<?, ?samples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_values = evaluate(\n",
    "    input_labels=r'path to the output of running ptn2gcn with split=val',\n",
    "    image_dir=r\"path to pubtabnet val images directory\",\n",
    "    detr_model=\"facebook/detr-resnet-50\",\n",
    "    checkpoint=r\"..\\checkpoint\\detr.ckpt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22622420393111212\n"
     ]
    }
   ],
   "source": [
    "print(sum(eval_values)/len(eval_values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
