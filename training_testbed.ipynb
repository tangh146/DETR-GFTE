{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-101-dc5 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DetrForObjectDetection were not initialized from the model checkpoint at facebook/detr-resnet-101-dc5 and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- class_labels_classifier.weight: found shape torch.Size([92, 256]) in the checkpoint and torch.Size([3, 256]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\tangy\\\\Downloads\\\\DETR-GFTE\\\\datasets\\\\ptn_examples_val\\\\ptn_examples_val.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      8\u001b[0m config \u001b[38;5;241m=\u001b[39m load_config(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigs\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtrain.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDetr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_gcn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_backbone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfacebook/detr-resnet-101-dc5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\remote desktop\\Downloads\\DETR-GFTE\\models.py:114\u001b[0m, in \u001b[0;36mDetr.__init__\u001b[1;34m(self, train_mode, with_gcn, lr, lr_backbone, weight_decay, pretrained, checkpoint, config)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCLASS_NO_RELATION, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCLASS_HORIZONTAL, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCLASS_VERTICAL, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCLASS_SAME_CELL \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_mode:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloaders \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataloaders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_data_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_labels_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_data_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_labels_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\remote desktop\\Downloads\\DETR-GFTE\\data.py:97\u001b[0m, in \u001b[0;36mcreate_dataloaders\u001b[1;34m(train_data_path, train_label_path, val_data_path, val_label_path, batch_size, image_processor)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_dataloaders\u001b[39m(train_data_path, train_label_path, val_data_path, val_label_path, batch_size, image_processor):\n\u001b[1;32m---> 97\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTrainDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_label_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_processor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m     val_dataset \u001b[38;5;241m=\u001b[39m TrainDataset(val_data_path, val_label_path, image_processor)\n\u001b[0;32m    100\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, collate_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m b: collate_fn(b, image_processor), batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\remote desktop\\Downloads\\DETR-GFTE\\data.py:22\u001b[0m, in \u001b[0;36mTrainDataset.__init__\u001b[1;34m(self, data_path, label_path, image_processor)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# TODO checkpoint? what if i am finetuning\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# self.image_processor = DetrImageProcessor.from_pretrained(checkpoint)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor \u001b[38;5;241m=\u001b[39m image_processor\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_offsets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_line_offsets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\remote desktop\\Downloads\\DETR-GFTE\\data.py:27\u001b[0m, in \u001b[0;36mTrainDataset._get_line_offsets\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_line_offsets\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     26\u001b[0m     offsets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m             offset \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mtell()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\tangy\\\\Downloads\\\\DETR-GFTE\\\\datasets\\\\ptn_examples_val\\\\ptn_examples_val.jsonl'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from models import Detr\n",
    "from utils import load_config\n",
    "import torch\n",
    "\n",
    "config = load_config(r\"configs\\train.yaml\")\n",
    "\n",
    "model = Detr(\n",
    "    train_mode=True,\n",
    "    with_gcn=True,\n",
    "    pretrained='facebook/detr-resnet-101-dc5',\n",
    "    checkpoint = None,\n",
    "    config=config)\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "# Set up checkpointing and trainer\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"validation/loss\",\n",
    "    filename=\"detr-{epoch:02d}-{validation/loss:.2f}\",\n",
    "    save_top_k=3,\n",
    "    mode=\"min\",\n",
    "    save_last=True,\n",
    "    dirpath=r\"checkpoints\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=50,\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    val_check_interval=0.5,  # Run validation 2 times per epoch\n",
    "    devices=1,\n",
    "    accelerator=\"gpu\",\n",
    "    gradient_clip_val=0.1,\n",
    "    accumulate_grad_batches=48\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                   | Params | Mode \n",
      "--------------------------------------------------------------------\n",
      "0 | model            | DetrForObjectDetection | 60.4 M | train\n",
      "1 | simplified_tbnet | SimplifiedTbNet        | 263 K  | train\n",
      "--------------------------------------------------------------------\n",
      "60.5 M    Trainable params\n",
      "222 K     Non-trainable params\n",
      "60.7 M    Total params\n",
      "242.822   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b745052451d46a28347fd88c5341774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'size': tensor([ 338, 1333]), 'image_id': tensor([38]), 'class_labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'boxes': tensor([[0.0581, 0.0750, 0.0867, 0.0833],\n",
      "        [0.4228, 0.0750, 0.0550, 0.0833],\n",
      "        [0.8161, 0.0750, 0.0761, 0.0833],\n",
      "        [0.3372, 0.1792, 0.0233, 0.0750],\n",
      "        [0.5074, 0.1792, 0.0761, 0.0750],\n",
      "        [0.7030, 0.1792, 0.0233, 0.0750],\n",
      "        [0.8816, 0.1792, 0.0761, 0.0750],\n",
      "        [0.0624, 0.3083, 0.0951, 0.1000],\n",
      "        [0.3372, 0.3083, 0.0529, 0.1000],\n",
      "        [0.5074, 0.3083, 0.1311, 0.1000],\n",
      "        [0.7030, 0.3083, 0.0529, 0.1000],\n",
      "        [0.8816, 0.3083, 0.1311, 0.1000],\n",
      "        [0.0624, 0.4083, 0.0951, 0.1000],\n",
      "        [0.3372, 0.4083, 0.0529, 0.1000],\n",
      "        [0.5074, 0.4083, 0.1311, 0.1000],\n",
      "        [0.7030, 0.4083, 0.0529, 0.1000],\n",
      "        [0.8816, 0.4083, 0.1311, 0.1000],\n",
      "        [0.1311, 0.5083, 0.2326, 0.1000],\n",
      "        [0.3383, 0.5083, 0.0423, 0.1000],\n",
      "        [0.5074, 0.5083, 0.1099, 0.1000],\n",
      "        [0.7030, 0.5083, 0.0402, 0.1000],\n",
      "        [0.8816, 0.5083, 0.1099, 0.1000],\n",
      "        [0.1353, 0.6167, 0.2410, 0.1000],\n",
      "        [0.3383, 0.6167, 0.0423, 0.1000],\n",
      "        [0.5074, 0.6167, 0.1099, 0.1000],\n",
      "        [0.7030, 0.6167, 0.0402, 0.1000],\n",
      "        [0.8816, 0.6167, 0.1099, 0.1000],\n",
      "        [0.1015, 0.7167, 0.1734, 0.1000],\n",
      "        [0.3383, 0.7167, 0.0423, 0.1000],\n",
      "        [0.5074, 0.7167, 0.1099, 0.1000],\n",
      "        [0.7030, 0.7167, 0.0402, 0.1000],\n",
      "        [0.8816, 0.7167, 0.1099, 0.1000],\n",
      "        [0.1290, 0.8250, 0.2283, 0.1000],\n",
      "        [0.3372, 0.8250, 0.0529, 0.1000],\n",
      "        [0.5074, 0.8250, 0.1311, 0.1000],\n",
      "        [0.7030, 0.8250, 0.0529, 0.1000],\n",
      "        [0.8816, 0.8250, 0.1311, 0.1000],\n",
      "        [0.1332, 0.9250, 0.2368, 0.1000],\n",
      "        [0.3372, 0.9250, 0.0529, 0.1000],\n",
      "        [0.5074, 0.9250, 0.1311, 0.1000],\n",
      "        [0.7030, 0.9250, 0.0529, 0.1000],\n",
      "        [0.8816, 0.9250, 0.1311, 0.1000]]), 'area': tensor([ 3254.5303,  2063.8484,  2857.6362,   785.8500,  2571.8726,   785.8500,\n",
      "         2571.8726,  4286.4546,  2381.3635,  5905.7817,  2381.3635,  5905.7817,\n",
      "         4286.4546,  2381.3635,  5905.7817,  2381.3635,  5905.7817, 10478.0000,\n",
      "         1905.0908,  4953.2363,  1809.8363,  4953.2363, 10859.0176,  1905.0908,\n",
      "         4953.2363,  1809.8363,  4953.2363,  7810.8726,  1905.0908,  4953.2363,\n",
      "         1809.8363,  4953.2363, 10287.4902,  2381.3635,  5905.7817,  2381.3635,\n",
      "         5905.7817, 10668.5088,  2381.3635,  5905.7817,  2381.3635,  5905.7817]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'orig_size': tensor([120, 473])}]\n",
      "[{'size': tensor([ 482, 1333]), 'image_id': tensor([15]), 'class_labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'boxes': tensor([[0.0525, 0.0988, 0.0966, 0.1047],\n",
      "        [0.2017, 0.0988, 0.1261, 0.1047],\n",
      "        [0.3950, 0.0988, 0.1849, 0.1047],\n",
      "        [0.5714, 0.0988, 0.0924, 0.1047],\n",
      "        [0.0315, 0.2558, 0.0546, 0.1163],\n",
      "        [0.1618, 0.2558, 0.0462, 0.1163],\n",
      "        [0.3824, 0.2558, 0.1597, 0.1163],\n",
      "        [0.7584, 0.3081, 0.4664, 0.2209],\n",
      "        [0.0315, 0.4186, 0.0546, 0.1163],\n",
      "        [0.1618, 0.4186, 0.0462, 0.1163],\n",
      "        [0.3824, 0.4186, 0.1597, 0.1163],\n",
      "        [0.0399, 0.5814, 0.0714, 0.1163],\n",
      "        [0.1660, 0.5814, 0.0546, 0.1163],\n",
      "        [0.3824, 0.5814, 0.1597, 0.1163],\n",
      "        [0.7479, 0.7384, 0.4454, 0.4302],\n",
      "        [0.0399, 0.7442, 0.0714, 0.1163],\n",
      "        [0.1660, 0.7442, 0.0546, 0.1163],\n",
      "        [0.3824, 0.7442, 0.1597, 0.1163]]), 'area': tensor([  6497.8867,   8475.5039,  12430.7393,   6215.3696,   4080.7983,\n",
      "          3452.9832,  11928.4873,  66203.1094,   4080.7983,   3452.9832,\n",
      "         11928.4873,   5336.4287,   4080.7983,  11928.4873, 123114.5469,\n",
      "          5336.4287,   4080.7983,  11928.4873]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'orig_size': tensor([ 86, 238])}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db21d0297e4a4ad9b16a4369cf3a9c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'size': tensor([ 722, 1333]), 'image_id': tensor([5]), 'class_labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'boxes': tensor([[0.0916, 0.0809, 0.1753, 0.0588],\n",
      "        [0.6693, 0.0809, 0.3187, 0.0588],\n",
      "        [0.1016, 0.2096, 0.1952, 0.0662],\n",
      "        [0.7092, 0.2096, 0.3984, 0.0662],\n",
      "        [0.1235, 0.2904, 0.2390, 0.0662],\n",
      "        [0.7012, 0.2904, 0.3825, 0.0662],\n",
      "        [0.0637, 0.3750, 0.1195, 0.0588],\n",
      "        [0.7072, 0.3750, 0.3944, 0.0588],\n",
      "        [0.0857, 0.4596, 0.1633, 0.0662],\n",
      "        [0.7092, 0.4596, 0.3984, 0.0662],\n",
      "        [0.0896, 0.5404, 0.1713, 0.0662],\n",
      "        [0.6952, 0.5404, 0.3705, 0.0662],\n",
      "        [0.1096, 0.6250, 0.2112, 0.0588],\n",
      "        [0.7012, 0.6250, 0.3825, 0.0588],\n",
      "        [0.0916, 0.7096, 0.1753, 0.0662],\n",
      "        [0.6813, 0.7096, 0.3426, 0.0662],\n",
      "        [0.1135, 0.7941, 0.2191, 0.0588],\n",
      "        [0.6912, 0.7941, 0.3625, 0.0588],\n",
      "        [0.0936, 0.8750, 0.1793, 0.0588],\n",
      "        [0.6992, 0.8750, 0.3785, 0.0588],\n",
      "        [0.1155, 0.9596, 0.2231, 0.0662],\n",
      "        [0.6992, 0.9596, 0.3785, 0.0662]]), 'area': tensor([ 9924.2432, 18044.0781, 12433.4980, 25374.4844, 15224.6914, 24359.5059,\n",
      "         6766.5293, 22329.5469, 10403.5391, 25374.4844, 10911.0283, 23598.2715,\n",
      "        11954.2021, 21652.8945, 11164.7734, 21822.0566, 12405.3037, 20525.1387,\n",
      "        10149.7939, 21427.3438, 14209.7119, 24105.7617]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'orig_size': tensor([136, 251])}]\n",
      "[{'size': tensor([ 482, 1333]), 'image_id': tensor([15]), 'class_labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'boxes': tensor([[0.0525, 0.0988, 0.0966, 0.1047],\n",
      "        [0.2017, 0.0988, 0.1261, 0.1047],\n",
      "        [0.3950, 0.0988, 0.1849, 0.1047],\n",
      "        [0.5714, 0.0988, 0.0924, 0.1047],\n",
      "        [0.0315, 0.2558, 0.0546, 0.1163],\n",
      "        [0.1618, 0.2558, 0.0462, 0.1163],\n",
      "        [0.3824, 0.2558, 0.1597, 0.1163],\n",
      "        [0.7584, 0.3081, 0.4664, 0.2209],\n",
      "        [0.0315, 0.4186, 0.0546, 0.1163],\n",
      "        [0.1618, 0.4186, 0.0462, 0.1163],\n",
      "        [0.3824, 0.4186, 0.1597, 0.1163],\n",
      "        [0.0399, 0.5814, 0.0714, 0.1163],\n",
      "        [0.1660, 0.5814, 0.0546, 0.1163],\n",
      "        [0.3824, 0.5814, 0.1597, 0.1163],\n",
      "        [0.7479, 0.7384, 0.4454, 0.4302],\n",
      "        [0.0399, 0.7442, 0.0714, 0.1163],\n",
      "        [0.1660, 0.7442, 0.0546, 0.1163],\n",
      "        [0.3824, 0.7442, 0.1597, 0.1163]]), 'area': tensor([  6497.8867,   8475.5039,  12430.7393,   6215.3696,   4080.7983,\n",
      "          3452.9832,  11928.4873,  66203.1094,   4080.7983,   3452.9832,\n",
      "         11928.4873,   5336.4287,   4080.7983,  11928.4873, 123114.5469,\n",
      "          5336.4287,   4080.7983,  11928.4873]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'orig_size': tensor([ 86, 238])}]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
